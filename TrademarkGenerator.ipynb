{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "TrademarkGenerator.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0QmvEkLOB1h",
        "colab_type": "text"
      },
      "source": [
        "# Trademark Generator\n",
        "A model to generate trademark names. The data comes from th [DPMA](https://register.dpma.de/DPMAregister/uebersicht). To get this install [git](https://git-scm.com/book/de/v1/Los-geht%E2%80%99s-Git-installieren) navigate to your desired directory with the `cd` command and use `git clone https://github.com/Xnartharax/MachineLearning-experiments`. To use this interactively install [anaconda](https://www.anaconda.com/distribution/#download-section) (The Python 3.7 version) and run `conda install pytorch` in the anaconda prompt. Then open a jupyter lab through the anaconda navigator and open the notebook from there."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-2bSsFiOB1m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import os\n",
        "from IPython.display import display, clear_output\n",
        "import pandas as pd\n",
        "import string\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from math import floor, ceil\n",
        "import sys\n",
        "import string\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.nn.utils.rnn import pack_sequence, pad_packed_sequence, pad_sequence, pack_padded_sequence\n",
        "plt.rcParams['figure.figsize'] = [9.5, 6]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hF-GEjpaOB1y",
        "colab_type": "text"
      },
      "source": [
        "## Utility functions\n",
        "Those are bunch of functions used to encode and decode the inputs and outputs of the network.\n",
        "### Char to onehot\n",
        "This converts a character into its onehot representation. Onehot vectors are used for multiclass input.All elements are zero except the one with the index of the correct label. E.g. we have the character `G`  the 7th letter in the alphabet so the onehot representation looks like `[0, 0, 0, 0, 0, 0, 1, 0, 0, ...]`\n",
        "### Sequence Encodeing\n",
        "This applies the aforementioned method to a string of characters. Since this function is only used for the input we only have to insert the Start-of-sequence-character (`<SOS>`). The last letter wich would be end-of-sequence (`<EOS>`) isn't passed as input.\n",
        "### Decoding target and output sequences\n",
        "This converts the numeric output of the network into human-readable text. We need two different functions here because target and output are differently formatted.  \n",
        "**TODO:** make a probabilistic sequence decoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "neX4ziyMOB11",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def char_to_onehot(char):\n",
        "    return [1 if char==alphabet[i] else 0 for i, _ in enumerate(alphabet)]\n",
        "\n",
        "def encode_sequence(sequence):\n",
        "    seq = list(sequence)\n",
        "    seq.insert(0, \"<SOS>\")\n",
        "    out = [char_to_onehot(char) for char in seq]\n",
        "    return out\n",
        "\n",
        "def decode_sequence(sequence):\n",
        "    chars = [alphabet[torch.argmax(char)] for char in sequence]\n",
        "    decoded = \"\"\n",
        "    for char in chars:\n",
        "        decoded += char\n",
        "    return decoded\n",
        "\n",
        "def decode_target_seq(target_seq):\n",
        "    decoded = \"\"\n",
        "    for t in target_seq:\n",
        "        decoded += alphabet[t.item()]\n",
        "    return decoded"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvxwCLSeOB19",
        "colab_type": "text"
      },
      "source": [
        "## Data inport\n",
        "See the [Pandas Documentation](https://pandas.pydata.org/pandas-docs/stable/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_oath3POB2D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c61a1c03-14d0-4255-da9a-3f8c6b83c3ab"
      },
      "source": [
        "df = pd.read_csv(\"https://github.com/Xnartharax/MachineLearning-experiments/raw/master/treffer.csv\", sep=';', error_bad_lines=False)\n",
        "\n",
        "df.drop(columns=[\"Datenbestand\", \"Aktenzeichen/Registernummer\", \"Aktenzustand\"], inplace=True)\n",
        "everything = []\n",
        "for _, row in df.iterrows():\n",
        "    everything += list(row[\"Markendarstellung\"].upper())\n",
        "alphabet = [\"<pad>\"] + list(set(everything)) + [\"<SOS>\", \"<EOS>\"]\n",
        "print(len(alphabet))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_1S6qqYOB2J",
        "colab_type": "text"
      },
      "source": [
        "## The data preprocessing\n",
        "Here the data is preprocessed for training. If you wonder what those loops in the brackets are: they are called [list comprehensions](https://www.pythonforbeginners.com/basics/list-comprehensions-in-python) (they are pretty useful and compact)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9T8L0mqROB2N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "features =[encode_sequence(seq[\"Markendarstellung\"].upper()) for i, seq in df.iterrows()]\n",
        "targets = [[alphabet.index(char) for char in list(seq[\"Markendarstellung\"].upper())]+[len(alphabet)-1]\\\n",
        "                  for i, seq in df.iterrows()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whivzkjtOB2U",
        "colab_type": "text"
      },
      "source": [
        "## Batches\n",
        "A batch is a small `sub-dataset` after wich an update is computed. This is useful because we don't have to pass the entire dataset for one update so we can accelerate the learning process. \n",
        "\n",
        "This method takes the feature and target dataset, a batch number i and the total number of batches as input. It then calculates a batch length and picks data areas. The batches are then sorted by sequence length because the packed sequences must be in that order.\n",
        "\n",
        "### Packed and padded sequences\n",
        "\n",
        "There are different methods to handle sequences in batches: Padding and Packing.\n",
        "\n",
        "#### Packed sequences\n",
        "***\n",
        "A packed sequences are fused together in a list. Additionally the lengths of the batch at each step are stored.\n",
        "\n",
        "**Example:**\n",
        "Consider the sequences: abc, abcdef, qrstu\n",
        "\n",
        "In packed form they look like: `aaqbbrccsdteuf`\n",
        "with the stored seperations: `3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 1`\n",
        "\n",
        "#### Padded sequences\n",
        "***\n",
        "Packed sequences have paddings inserted at the end to bring them all up to equal length.\n",
        "\n",
        "**Example:**\n",
        "Same sequences as before: abc, abcdef, qrstu\n",
        "\n",
        "padded they look like `abc<pad><pad><pad>, abcdef, qrstu<pad>`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9S71vlZOB2W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_batch(features, targets, i, n_batches):\n",
        "    batch_len = int(floor(len(features)/n_batches))\n",
        "    \n",
        "    batch_features = sorted(features[i*batch_len:(i+1)*batch_len],\n",
        "                            key=lambda x: len(x), reverse=True)\n",
        "    batch_targets = sorted(targets[i*batch_len:(i+1)*batch_len],\n",
        "                           key=lambda x: len(x), reverse=True)\n",
        "    padded_features = pad_sequence([torch.tensor(seq, dtype=torch.float32).view(-1, len(alphabet))\\\n",
        "                                        for seq in batch_features])\n",
        "    padded_targets = pad_sequence([torch.tensor(target) for target in batch_targets])\n",
        "    if torch.cuda.is_available():\n",
        "      return (padded_features.cuda(),\\\n",
        "            padded_targets.cuda(), batch_len)\n",
        "    else:\n",
        "      return (padded_features,\\\n",
        "            padded_targets, batch_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjFGjs7IOB2a",
        "colab_type": "text"
      },
      "source": [
        "## The LSTM Wrapper\n",
        "This class wraps things like [residual shortcuts](https://towardsdatascience.com/residual-blocks-building-blocks-of-resnet-fd90ca15d6ec), dropout and batch normalization around an LSTM. The weight initialization is also important. Otherwise the network fails tolearn. The weights are sampled from a normal distribution with a mean equal to: $\\frac{1}{\\sqrt{N_{input}}}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6R8hxB8OOB2i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LSTMBlock(nn.Module):\n",
        "    def __init__(self, in_size, out_size, residual=False, \n",
        "                 dropout_p=0, layer_norm=False):\n",
        "        super(LSTMBlock, self).__init__()\n",
        "        self.core = nn.LSTM(in_size, out_size)\n",
        "        # weight init\n",
        "        y = 1/np.sqrt(in_size)\n",
        "        self.core.weight_ih_l0.data.normal_(y)\n",
        "        self.core.weight_ih_l0.data.normal_(y)\n",
        "        # residual shortcuts\n",
        "        self.residual = residual\n",
        "        if self.residual:\n",
        "            self.resShortcut = nn.Linear(in_size, out_size)\n",
        "        # batch norm\n",
        "        self.norm = nn.LayerNorm(out_size) if layer_norm else nn.Identity()\n",
        "        # dropout\n",
        "        self.dropout = nn.Dropout(p=0)\n",
        "        \n",
        "    def forward(self, inputs, hidden):\n",
        "        out, hidden = self.core(inputs, hidden)\n",
        "        if self.residual:\n",
        "            # transforming the input dimensions to match the output dimensions\n",
        "            inputs_transformed = self.resShortcut(inputs)\n",
        "            out = out + inputs_transformed\n",
        "        out= self.norm(out)\n",
        "        return out, hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgqjpBpMOB2u",
        "colab_type": "text"
      },
      "source": [
        "## The Char Predictor Class\n",
        "Pretty obvious: given a sequence of characters the Char preditor will compute likelihoods of the next chars. It is the  core part for generating a sequence \n",
        "### Constructor\n",
        "****\n",
        "It takes a tuple of integers as argument. This is a convenient way to quickly redefine the network architecture. All layers except the last will be LSTM layers. The last layer is a straightforward linear layer on wich a softmax follows. The Softmax normalizes all outputs to sum up to 1 and exponentially magnifies the most promisiing outputs. The outputs can be interpreted as likelihoods because they sum up to one.\n",
        "$$softmax_i(x) = \\frac{e^{x_i}} {\\sum \\limits_{j} e^{x_j}}$$\n",
        "### Forward-Passing\n",
        "****\n",
        "We use  to prevent a vanishing gradient. Since linear layers don't work with packed sequences, the data has to be converted into a padded sequence before propagating to the final layer.\n",
        "### Training\n",
        "****\n",
        "Probably the most convoluted and messy method due to constant tinkering. It uses mini-batch learning wich means you have two loops: one for the epochs, one for the batches. In each epoch we determine how many batches to use wich determines how big the batches are. Increasing the size of the batches during training is an alternative to decreasing the learning rate because larger batches mean more accurate descent. First thing to do in the batch loop is to set the gradients to zero because torch accumulates gradients otherwise. After that we get a data batch on wich we compute our preditions and evaluate a given loss function. `loss.backward()` computes the gradients used by `optim.step()`. After that an evaluation loss is computed with a seperate evaluation dataset. If the evaluation loss is smaller than 3 and the currently best loss a checkpoint is saved.\n",
        "\n",
        "**TODO:** \n",
        "1. Accuracy evaluation\n",
        "2. Seperate the Training process into a Trainer class\n",
        "3. Directly calculate batch length and take n_batches from there\n",
        "\n",
        "### Checkpoints\n",
        "****\n",
        "see [the Documentation](https://pytorch.org/tutorials/beginner/saving_loading_models.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XPP7zzTOB2y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CharPredictor(nn.Module):\n",
        "    def __init__(self, structure, dropout_p=0.1, residual=False,\n",
        "                 layer_norm=False, hidden_init=torch.zeros):\n",
        "        super(CharPredictor, self).__init__()\n",
        "        # important meta-information\n",
        "        self.structure = structure\n",
        "        self.layers = len(structure)\n",
        "        self.hidden_init = hidden_init\n",
        "        # the actual network\n",
        "        self.lstms = nn.ModuleList([LSTMBlock(i, o, dropout_p=dropout_p,                                              \n",
        "                                    residual=False, layer_norm=layer_norm)\n",
        "                                    for i, o in zip(structure[:-2], structure[1:-1])])\n",
        "        self.final = nn.Linear(structure[-2], structure[-1])\n",
        "        self.final.weight.data.normal_(1/np.sqrt(structure[-1]))\n",
        "        self.activation = nn.Softmax(dim=2)\n",
        "        \n",
        "    def forward(self, seq, keep_hidden_state=False, batch_size=1, plots={}):\n",
        "        # diagnostic plots please ignore\n",
        "        if \"activations\" in plots:\n",
        "            means = []\n",
        "        if \"stds\" in plots:\n",
        "            stds = []\n",
        "        if \"variances\" in plots:\n",
        "            variances = []\n",
        "            \n",
        "        # if you want to pass single characters enable keeping the hidden state of the LSTMs\n",
        "        # otherwise the network assumes that each forward pass is a new sequence\n",
        "        if not keep_hidden_state:\n",
        "            self.init_hidden(batch_size)\n",
        "            \n",
        "        # important part   \n",
        "        inputs = seq\n",
        "        for i, lstm in enumerate(self.lstms):\n",
        "            # using output of the previous layer as input for all layers but the first\n",
        "            out, self.hidden[i] = lstm(inputs, self.hidden[i]) \n",
        "            inputs = out\n",
        "            #more monitoring plots\n",
        "            if \"activations\" in plots:\n",
        "                means.append(out.data.abs().mean().item())  \n",
        "            if \"stds\" in plots:\n",
        "                stds.append(out.data.std().item())\n",
        "            if \"variances\" in plots:\n",
        "                variances.append(out.data.var().item())\n",
        "                \n",
        "        # fancy data reformatting because Dense layers can't handle packed sequences\n",
        "\n",
        "        out = self.activation(self.final(out))\n",
        "        \n",
        "        \n",
        "        # even more dagnostic plot stuff\n",
        "        if \"activations\" in plots:\n",
        "            means.append(out.mean().item())\n",
        "            plots[\"activations\"].set_data(range(len(self.structure)-1), means)\n",
        "        if \"stds\" in plots:\n",
        "            stds.append(out.std().item())\n",
        "            plots[\"stds\"].set_data(range(len(self.structure)-1), stds)\n",
        "        if \"variances\" in plots:\n",
        "            variances.append(out.var().item())\n",
        "            plots[\"variances\"].set_data(range(len(self.structure)-1), variances)\n",
        "        \n",
        "        return out\n",
        "    \n",
        "    def fit(self, seqs, targets, eval_seqs, eval_targets, epochs, n_batches, \n",
        "            optim, criterion,\n",
        "              plots={}, start_epoch=0, losses=[], eval_losses=[]):\n",
        "        best_loss = 100\n",
        "        # preparing the evaluation data\n",
        "        eval_X, eval_y, eval_batch_len = get_batch(eval_seqs, eval_targets, 0, 1)\n",
        "        \n",
        "        for epoch in range(start_epoch, epochs):\n",
        "            # decreasing the number of batches with each epoch\n",
        "            N_batches = ceil(n_batches * (0.99**epoch))\n",
        "            for batch in range(N_batches):\n",
        "                model.zero_grad()\n",
        "                \n",
        "                # getting the batch data\n",
        "                X, y, batch_len = get_batch(seqs, targets, batch, N_batches)\n",
        "                \n",
        "                # computing the Predictions\n",
        "                y_hat = self(X, plots=plots, batch_size=batch_len)\n",
        "                \n",
        "                # computing the loss\n",
        "                loss = criterion(y_hat, y)\n",
        "                \n",
        "                # computing the gradients\n",
        "                loss.backward()\n",
        "                \n",
        "                #  computing evaluation predictions and loss\n",
        "                eval_y_hat = self(eval_X, batch_size=eval_batch_len)\n",
        "                eval_loss = criterion(eval_y_hat, eval_y)\n",
        "                \n",
        "                # plotting stuff\n",
        "                if \"grads\" in plots:\n",
        "                    grads = [lstm.core.weight_ih_l0.grad.abs().mean() + \\\n",
        "                             lstm.core.weight_hh_l0.grad.abs().mean()\n",
        "                             for lstm in self.lstms] \\\n",
        "                    + [self.final.weight.grad.mean()]\n",
        "                    \n",
        "                #applying the updates\n",
        "                optim.step() \n",
        "                \n",
        "                # monitoring\n",
        "                clear_output()\n",
        "                print(f'{epoch}:{batch}/{N_batches}|{loss.cpu().detach().numpy()}|{eval_loss.cpu().detach().numpy()}' )\n",
        "                losses.append(loss.cpu().detach().numpy())\n",
        "                eval_losses.append(eval_loss.cpu().detach().numpy())\n",
        "                if plots != {}:\n",
        "                    if \"loss\" in plots:\n",
        "                        plots[\"loss\"].set_data(range(len(losses)), losses)\n",
        "                    if \"grads\" in plots:\n",
        "                        plots[\"grads\"].set_data(range(len(self.structure)-1),\n",
        "                                                grads)\n",
        "                    if \"eval_loss\" in plots:\n",
        "                        plots[\"eval_loss\"].set_data(range(len(eval_losses)),\n",
        "                                                    eval_losses)\n",
        "                    display(plots[\"fig\"])\n",
        "                \n",
        "                # saving the checkpoints\n",
        "                # if(eval_loss<3 and eval_loss<best_loss):\n",
        "                #     self.save_checkpoint(losses, eval_losses, epoch, optim)\n",
        "                #     best_loss = eval_loss.detach()\n",
        "                #     print(\"saved checkpoint\")\n",
        "                    \n",
        "    def append_layer(self, n_neurons, dropout_p=0, residual=False,\n",
        "                     batch_norm=False):\n",
        "        self.lstms.append(LSTMBlock(self.structure[-2], n_neurons,\n",
        "                                    dropout_p=dropout_p, residual=False,\n",
        "                                    batch_norm=batch_norm))\n",
        "        temp = list(self.structure)\n",
        "        temp.insert(-1, n_neurons)\n",
        "        self.structure= tuple(temp)\n",
        "        self.layers+=1\n",
        "        self.final = nn.Linear(n_neurons, self.structure[-1])\n",
        "                    \n",
        "    def init_hidden(self, batches):\n",
        "        if torch.cuda.is_available:\n",
        "            self.hidden = [(self.hidden_init(1, batches, layer_size).cuda(),\n",
        "                       self.hidden_init(1, batches, layer_size).cuda())\n",
        "                           for layer_size in self.structure[1:]] \n",
        "        else:\n",
        "            self.hidden = [(self.hidden_init(1, batches, layer_size),\n",
        "                       self.hidden_init(1, batches, layer_size))\n",
        "                           for layer_size in self.structure[1:]] \n",
        "        \n",
        "    def save_checkpoint(self, losses, eval_losses, epoch, optim):\n",
        "        if os.path.exists(\"./safepoints/safepoint.pt\"):\n",
        "            os.remove(\"./safepoints/safepoint.pt\")\n",
        "        torch.save({\n",
        "            \"model_state_dict\":self.state_dict(),\n",
        "            \"optim_state_dict\":optim.state_dict(),\n",
        "            \"losses\":losses,\n",
        "            \"eval_losses\":eval_losses,\n",
        "            \"epoch\":epoch,\n",
        "        },f\"./safepoints/safepoint.pt\")\n",
        "    def resume_from_checkpoint(self, seqs, targets, eval_seqs, eval_targets,epochs, n_batches, optim, criterion,\n",
        "                               plots={}, checkpoint='./safepoints/safepoint.pt'):\n",
        "        state_dict = torch.load(checkpoint)\n",
        "        self.load_state_dict(state_dict[\"model_state_dict\"])\n",
        "        optim.load_state_dict\n",
        "        self.fit(seqs, targets, eval_seqs, eval_targets, epochs, n_batches, optim, criterion, plots=plots,\n",
        "                   start_epoch=state_dict[\"epoch\"], losses=state_dict[\"losses\"], eval_losses=state_dict[\"eval_losses\"])\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZ84Q7A1OB23",
        "colab_type": "text"
      },
      "source": [
        "## Pre-Training definitions\n",
        "Structure definition an model initialization should be clear. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdxnuC14OB24",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = CharPredictor((len(alphabet), 200, 200, len(alphabet)))\n",
        "if torch.cuda.is_available():\n",
        "  model.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3e9IyiRTOB3A",
        "colab_type": "text"
      },
      "source": [
        "## Training Parameters\n",
        "Here we choose Optimizer, Loss function and number of epochs and batches\n",
        "### Loss Function: The Negative Log likelihood Loss (NLL)\n",
        "This is the standard for multiclass sequence regression ([more Info](https://pytorch.org/docs/stable/nn.html#nllloss)). It is important to weigh the extra symbols less because they are more frequent.  \n",
        "The number of occurencies of the padding should roughly be half the average size of sequences, `<SOS>` is never in the target sequence and `<EOS>` appears in every target sequence\n",
        "### The Adam Optimizer\n",
        "See [great source for optimizers](http://ruder.io/optimizing-gradient-descent/).\n",
        "### Batches and Epochs\n",
        "The batches are set to contain roughly 200 or more data points each and the epochs are far more than actually needed (i think)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShvbHrbIOB3E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optim = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "occurencies = [everything.count(char)for char in alphabet[1:-2]]\n",
        "occurencies = [occ if occ>10 else 100000 for occ in occurencies] # a 1 causes the inverse log to be infinite and we dont want the rare letters in there\n",
        "weights = [torch.tensor([len(seq) for seq in features], dtype=torch.float).mean().item()*len(features)]\\\n",
        "        + occurencies + [0, len(features)] \n",
        "lossweight = 1/torch.log(torch.tensor(weights))\n",
        "if torch.cuda.is_available():\n",
        "    lossweight = lossweight.cuda()\n",
        "loss_func = nn.NLLLoss(weight = lossweight)\n",
        "epochs = 1000\n",
        "n_batches = 99\n",
        "def criterion(y_hat, y):\n",
        "    main_loss = loss_func(torch.log(y_hat).view(-1, len(alphabet)), y.view(-1))\n",
        "    loss = main_loss# + weight_punishment**0.0002 + 1*0.000001/y_hat.var()\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQKmBDvROB3H",
        "colab_type": "text"
      },
      "source": [
        "## Training and monitoring\n",
        "This starts the training loop. It displays some metrics of the learning progress. In the top left corner the training loss (blue) and the evaluation loss (red) are shown.  \n",
        "In the top right you can see the average magnitude of the activation (red) the standard deviations (blue) and the variances (green) per layer. Because of the softmax the activation average will be fixed but standard deviation and variance should be higher since that means the model is deciding for a specific label.  \n",
        "In the bottom left there is the average magnitude of the gradient per layer. This shows how much each layer is adjusted.  \n",
        "The bottom right is currently empty. This is where an accuracy plot could go.  \n",
        "You don't really have to understand how it works but if you really want to here is the [matplotlib documentation](https://matplotlib.org/).  \n",
        "**BUG:** It doesn't clear the plot when reinitializing the model or restarting the learning process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaaci8OUOB3I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, ((lax, aax), (gax, accax)) = plt.subplots(ncols=2, nrows=2)\n",
        "plots = {\n",
        "    \"fig\":fig, \n",
        "    \"activations\":aax.plot([0, len(model.structure)-2], [0, 1], \"r\")[0],\n",
        "    \"stds\":aax.plot([0, len(model.structure)-2], [0, 1], \"b\")[0],\n",
        "    \"variances\":aax.plot([0, len(model.structure)-2], [0, 1], \"g\")[0],\n",
        "    \"loss\":lax.plot([0, epochs*n_batches], [0, 5], \"b\")[0],\n",
        "    \"eval_loss\":lax.plot([0, epochs*n_batches], [0,  5], \"r\")[0],\n",
        "    \"grads\":gax.plot([0, len(model.structure)-2], [0, 1])[0],\n",
        "#     \"accuracy\":accax.plot([0, epochs*n_batches], [0, 1])\n",
        "}  \n",
        "model.train()\n",
        "model.fit(features[100:10000], targets[100:10000], features[:100], targets[:100], epochs, n_batches, optim, criterion, plots=plots)\n",
        "torch.save(model, \"./good_try.pt\") "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5AGoTowOB3M",
        "colab_type": "code",
        "colab": {},
        "outputId": "f25291e3-3021-4579-e36f-7ad6231d324a"
      },
      "source": [
        "ins, target, batch_len = get_batch(features, targets, 110, 1000)\n",
        "model.eval()\n",
        "print(model.training)\n",
        "\n",
        "print(decode_sequence(model(ins, batch_size=10)[:, 2, :]))\n",
        "print(decode_target_seq(target[:, 2]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False\n",
            "STAR SHAA<EOS><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "STAR TREK<EOS><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLqqkBlSOB3U",
        "colab_type": "text"
      },
      "source": [
        "## How to go on\n",
        "The Character prediction is just the core part of the Generator. We still need to think about thinks like the starting input for generation (the seed), the output sampling method and a general application of this thing. This [Medium article ](https://towardsdatascience.com/the-arti-canon-neural-text-generation-2a8f032c2a68) gives a broader overview. If you want to learn how everything works you can take on the TODOs. Here are some things I'd like to try for the Predictor:\n",
        "- Train on a small dataset. Then add some more data. Train again. Add more data and so on...\n",
        "- Adding more layers through time.\n",
        "- Regularization (Dropout, L1, L2, etc.)\n",
        "- Look into [Transformers](https://arxiv.org/pdf/1706.03762.pdf)/Attention\n",
        "- Experiment with other datasets/more context\n",
        "- more data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEG1tXHoOB3V",
        "colab_type": "text"
      },
      "source": [
        "## Saving the model\n",
        "Please have a coherent naming convention. Name structure and methods used for training like dropout and residual shortcuts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjPy2U1QOB3X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(model.state_dict(), './200-200-dropout.tsd')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}